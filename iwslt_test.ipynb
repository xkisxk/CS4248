{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"iwslt2017\"\n",
    "dataset = load_dataset(dataset_name, \"iwslt2017-zh-en\", cache_dir=\"./cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opencc\n",
    "converter = opencc.OpenCC('s2t.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# Create dict for text into strokes translation and vice versa\n",
    "with open(\"./vocab/zh2letter.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    conversions = f.read()\n",
    "\n",
    "conversions = conversions.splitlines()\n",
    "dic = defaultdict(str)\n",
    "stroke2word = defaultdict(str)\n",
    "for line in conversions:\n",
    "    chinese_char, strokes = line.split()\n",
    "    dic[chinese_char] = strokes\n",
    "    stroke2word[strokes] = chinese_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strokify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def is_chinese(uchar):\n",
    "    \"\"\"判断一个unicode是否是汉字\"\"\"\n",
    "    if (uchar >= u'\\u4e00') and (uchar <= u'\\u9fa5'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def zh2letter(dictionary, line):\n",
    "    char_set = set(list(line))\n",
    "    newline = line\n",
    "    for char in char_set:\n",
    "        if is_chinese(char):\n",
    "            newline = newline.replace(char, ' '+dictionary.get(char, '')+' ')\n",
    "    return ' '.join(newline.split())+'\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPES = [\"zh\", \"tz\"]\n",
    "NAMES = [\"simp\", \"trad\"]\n",
    "TYPE = 0 # 0 for simplified, 1 for traditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split=\"test\"\n",
    "if TYPE == 0:\n",
    "    src_text = [pair[\"zh\"] for pair in dataset[split][\"translation\"]]\n",
    "else:\n",
    "    src_text = [converter.convert(pair[\"zh\"]) for pair in dataset[split][\"translation\"]]\n",
    "trg_text = [pair[\"en\"] for pair in dataset[split][\"translation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "src = TYPES[TYPE]\n",
    "trg = \"en\"\n",
    "\n",
    "func = partial(zh2letter, dic)\n",
    "iter = map(func, src_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"./data/NIST/{NAMES[TYPE]}/all\"\n",
    "with open(f\"{path}/{split}.{src}-{trg}.{src}\", 'w', encoding=\"utf-8\") as f:\n",
    "    for k in tqdm(iter): f.write(k)\n",
    "\n",
    "with open(f\"{path}/{split}.{src}-{trg}.{trg}\", 'w', encoding=\"utf-8\") as f:\n",
    "    for k in tqdm(trg_text): f.write(f\"{k}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split by average Token length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_split = []\n",
    "zh_split = []\n",
    "for pair in dataset[\"test\"][\"translation\"]:\n",
    "    en_split.append(pair[\"en\"])\n",
    "    zh_split.append(pair[\"zh\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "TYPES = [\"zh\", \"tz\"]\n",
    "NAMES = [\"simp\", \"trad\"]\n",
    "TYPE = 0 # 0 for simplified, 1 for traditional\n",
    "\n",
    "src = TYPES[TYPE]\n",
    "trg = \"en\"\n",
    "\n",
    "func = partial(zh2letter, dic)\n",
    "iter = map(func, zh_split)\n",
    "\n",
    "strokes = []\n",
    "for k in tqdm(iter):\n",
    "    strokes.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "avg_token_len = []\n",
    "for sent in strokes:\n",
    "    words = sent.split(\" \")\n",
    "    stroke_len = [len(word) for word in words]\n",
    "    avg_token_len.append(np.average(stroke_len))\n",
    "for p in [33, 66]:\n",
    "    print(np.percentile(avg_token_len, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_avg_strokes = defaultdict(list)\n",
    "trg_avg_strokes = defaultdict(list)\n",
    "for i, l in enumerate(avg_token_len):\n",
    "    if l <= 6.531808510638299:\n",
    "        src_avg_strokes[\"short\"].append(zh_split[i])\n",
    "        trg_avg_strokes[\"short\"].append(en_split[i])\n",
    "    elif l <= 7.136363636363637:\n",
    "        src_avg_strokes[\"medium\"].append(zh_split[i])\n",
    "        trg_avg_strokes[\"medium\"].append(en_split[i])\n",
    "    else:\n",
    "        src_avg_strokes[\"long\"].append(zh_split[i])\n",
    "        trg_avg_strokes[\"long\"].append(en_split[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "src = TYPES[TYPE]\n",
    "trg = \"en\"\n",
    "\n",
    "func = partial(zh2letter, dic)\n",
    "iter = map(func, src_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split==\"test\":\n",
    "    path = f\"C:/Users/xk_20/Documents/Code/CS4248/StrokeNet/data/NIST/{NAMES[TYPE]}/test/sent\"\n",
    "else:\n",
    "    path = f\"C:/Users/xk_20/Documents/Code/CS4248/StrokeNet/data/NIST/{NAMES[TYPE]}\"\n",
    "# with open(f\"{path}/{split}.{src}-{trg}.{src}\", 'w', encoding=\"utf-8\") as f:\n",
    "#     for k in tqdm(iter): f.write(k)\n",
    "\n",
    "# with open(f\"{path}/{split}.{src}-{trg}.{trg}\", 'w', encoding=\"utf-8\") as f:\n",
    "#     for k in tqdm(trg_text): f.write(f\"{k}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split by length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_split = []\n",
    "zh_split = []\n",
    "for pair in dataset[\"test\"][\"translation\"]:\n",
    "    en_split.append(pair[\"en\"])\n",
    "    zh_split.append(converter.convert(pair[\"zh\"]))\n",
    "lens = [len(zh) for zh in zh_split]\n",
    "import numpy as np\n",
    "for p in [33, 66]:\n",
    "    print(np.percentile(lens, p))\n",
    "# Split by length\n",
    "from collections import defaultdict\n",
    "sentence_by_length = defaultdict(list)\n",
    "for pair in dataset[\"test\"][\"translation\"]:\n",
    "    if len(pair[\"zh\"]) <= 18:\n",
    "        sentence_by_length[\"short\"].append(pair)\n",
    "    elif len(pair[\"zh\"]) <= 33:\n",
    "        sentence_by_length[\"medium\"].append(pair)\n",
    "    else:\n",
    "        sentence_by_length[\"long\"].append(pair)\n",
    "for type, sent in sentence_by_length.items():\n",
    "    print(type, len(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split=\"test\"\n",
    "\n",
    "src_text = defaultdict(list)\n",
    "trg_text = defaultdict(list)\n",
    "for type, sent in sentence_by_length.items():\n",
    "    src_text[type] = [pair[\"zh\"] for pair in sent]\n",
    "    trg_text[type] = [pair[\"en\"] for pair in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split==\"test\":\n",
    "    path = f\"./data/NIST/{NAMES[TYPE]}/test/sent\"\n",
    "else:\n",
    "    path = f\"./data/NIST/{NAMES[TYPE]}\"\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sentence_by_length.keys():\n",
    "    iter = map(func, src_text[word])\n",
    "    with open(f\"{path}/{split}-{word}.{src}-{trg}.{src}\", 'w', encoding=\"utf-8\") as f:\n",
    "        for k in tqdm(iter): f.write(k)\n",
    "\n",
    "    with open(f\"{path}/{split}-{word}.{src}-{trg}.{trg}\", 'w', encoding=\"utf-8\") as f:\n",
    "        for k in tqdm(trg_text[word]): f.write(f\"{k}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finer granularity of average token length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "percentiles = defaultdict(int)\n",
    "percentiles[0] = 0\n",
    "for p in range(5, 100, 5):\n",
    "    percentiles[p] = np.percentile(avg_token_len, p)\n",
    "percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by length\n",
    "from collections import defaultdict\n",
    "from bisect import bisect_left\n",
    "sentence_by_length2 = defaultdict(list)\n",
    "for pair in dataset[\"test\"][\"translation\"]:\n",
    "    id = bisect_left(list(percentiles.values()), len(pair[\"zh\"])) - 1\n",
    "    p = list(percentiles.keys())[id]\n",
    "    sentence_by_length2[p].append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from bisect import bisect_left\n",
    "avg_strokes = defaultdict(list)\n",
    "test = dataset[\"test\"][\"translation\"]\n",
    "for i, l in enumerate(avg_token_len):\n",
    "    id = bisect_left(list(percentiles.values()), l) - 1\n",
    "    p = list(percentiles.keys())[id]\n",
    "    avg_strokes[p].append(test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_strokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split=\"test\"\n",
    "\n",
    "src_text2 = defaultdict(list)\n",
    "trg_text2 = defaultdict(list)\n",
    "for type, sent in avg_strokes.items():\n",
    "    src_text2[type] = [pair[\"zh\"] for pair in sent]\n",
    "    trg_text2[type] = [pair[\"en\"] for pair in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"C:/Users/xk_20/Documents/Code/CS4248/StrokeNet/data/NIST/simp_original/test/sent_fine\"\n",
    "split=\"test\"\n",
    "for word in avg_strokes.keys():\n",
    "    iter = map(func, src_text2[word])\n",
    "    with open(f\"{path}/{split}-{word}.{src}-{trg}.{src}\", 'w', encoding=\"utf-8\") as f:\n",
    "        for k in tqdm(iter): f.write(k)\n",
    "\n",
    "    with open(f\"{path}/{split}-{word}.{src}-{trg}.{trg}\", 'w', encoding=\"utf-8\") as f:\n",
    "        for k in tqdm(trg_text2[word]): f.write(f\"{k}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".TFT_OCR_BOT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
