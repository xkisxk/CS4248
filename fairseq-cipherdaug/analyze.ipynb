{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../chinese_freq.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "freqs = []\n",
    "words = []\n",
    "translations = []\n",
    "for line in lines:\n",
    "    chunks = line.split(\"\\t\")\n",
    "    words.append(chunks[1])\n",
    "    freqs.append(chunks[2])\n",
    "    translations.append(chunks[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../frequency.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "freqs = []\n",
    "words = []\n",
    "for line in lines:\n",
    "    chunks = line.split()\n",
    "    words.append(chunks[0])\n",
    "    freqs.append(chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_25 = 33+1\n",
    "top_50 = 152+1\n",
    "top_75 = 483+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# Create dict for text into strokes translation and vice versa\n",
    "with open(\"../vocab/zh2letter.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    conversions = f.read()\n",
    "\n",
    "conversions = conversions.splitlines()\n",
    "dic = defaultdict(str)\n",
    "st2zh = defaultdict(str)\n",
    "for line in conversions:\n",
    "    chinese_char, strokes = line.split()\n",
    "    dic[chinese_char] = strokes\n",
    "    st2zh[strokes] = chinese_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===Jit: state[\"model\"] does not contain key \"_metadata\"=====\n",
      "===Jit: we will be filling in with current model's meta-data instead.\n"
     ]
    }
   ],
   "source": [
    "from fairseq.models.lstm import LSTMModel\n",
    "path = \"C:/Users/xk_20/Documents/Code/CS4248/StrokeNet/checkpoints/NIST/lstm_trad\"\n",
    "bpe_path = \"C:/Users/xk_20/Documents/Code/CS4248/StrokeNet/data/NIST/trad/bpe\"\n",
    "lstm_trad = LSTMModel.from_pretrained(\n",
    "  path,\n",
    "  checkpoint_file='checkpoint_best.pt',\n",
    "  data_name_or_path=bpe_path + '/bin',\n",
    "  bpe='subword_nmt',\n",
    "  bpe_codes=bpe_path + '/joint.code',\n",
    "  lang_dict=bpe_path + '/bin/langs.file'\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===Jit: state[\"model\"] does not contain key \"_metadata\"=====\n",
      "===Jit: we will be filling in with current model's meta-data instead.\n"
     ]
    }
   ],
   "source": [
    "from fairseq.models.lstm import LSTMModel\n",
    "path = \"C:/Users/xk_20/Documents/Code/CS4248/StrokeNet/checkpoints/NIST/lstm_simp\"\n",
    "bpe_path = \"C:/Users/xk_20/Documents/Code/CS4248/StrokeNet/data/NIST/simp_original/bpe\"\n",
    "lstm_simp = LSTMModel.from_pretrained(\n",
    "  path,\n",
    "  checkpoint_file='checkpoint_best.pt',\n",
    "  data_name_or_path=bpe_path + '/bin',\n",
    "  bpe='subword_nmt',\n",
    "  bpe_codes=bpe_path + '/joint.code',\n",
    "  lang_dict=bpe_path + '/bin/langs.file'\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opencc\n",
    "converter = opencc.OpenCC('s2t.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eaioteatn eadeaseeao ooheeaaetnerto '"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"柬埔寨\"\n",
    "strokes = \"\"\n",
    "for word in sentence:\n",
    "    strokes += dic[word] + \" \"\n",
    "strokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'taetno teadeaeero etaeae   eaaef lldiec ohatost lthno aieetsee hr aieec oodsac tteaie  thnaie aieeaeoe tatc ootohhre oohetktttn lldtnst lldtnst teatoar tseea aieec oodsac tnst thtseeastteea rhtn etsuto eadst  eaaaietn aieeaeoe tatc ootohhre oohetktttn oodhn iebta aieeaee1  eaeaieaeeaaetouto lldtaoeeeae aahneae erdeaeero etao ooathtseeastteea eadsac totn1 eaieieer eer1 teatoar tseea aieec oodsac tnst thtseeastteea thnaie teatoaiea teadtstt tteaaito taieetso oohooeto ibdtneoote etaieeeta oohtotv  '"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"钠是一种金属元素\"\n",
    "sentence = \"伏特在19世纪初发明了电池后，各国化学家纷纷利用电池分解水成功。英国化学家汉弗里·戴维坚持不懈地从事于利用电池分解各种物质的实验研究。\"\n",
    "# sentence = converter.convert(sentence)\n",
    "strokes = \"\"\n",
    "for word in sentence:\n",
    "    strokes += dic[word] + \" \"\n",
    "strokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = lstm_simp.apply_bpe(strokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simp_many = []\n",
    "for i,word in enumerate(words):\n",
    "    strokes = dic[word]\n",
    "    print(word, lstm_simp.apply_bpe(strokes))\n",
    "    tok = lstm_simp.apply_bpe(strokes)\n",
    "    if len(tok.split(\" \")) > 1:\n",
    "        simp_many.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trad_many = []\n",
    "for i, word in enumerate(words):\n",
    "    word = converter.convert(word)\n",
    "    strokes = dic[word]\n",
    "    print(word, lstm_trad.apply_bpe(strokes))\n",
    "    tok = lstm_trad.apply_bpe(strokes)\n",
    "    if len(tok.split(\" \")) > 1:\n",
    "        trad_many.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ttsoe@@ oeaie@@ aaeaiee'"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_trad.apply_bpe(dic[converter.convert(\"艚\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ttsoeo'"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_simp.apply_bpe(dic[\"舟\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'昔'"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st2zh[\"eaaeaiee\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'鎧'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.convert(\"铠\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2554,\n",
       " 2709,\n",
       " 2730,\n",
       " 2735,\n",
       " 2830,\n",
       " 2877,\n",
       " 2879,\n",
       " 2882,\n",
       " 2908,\n",
       " 2962,\n",
       " 2993,\n",
       " 3029,\n",
       " 3030,\n",
       " 3121,\n",
       " 3128,\n",
       " 3156,\n",
       " 3161,\n",
       " 3178,\n",
       " 3196,\n",
       " 3227,\n",
       " 3261,\n",
       " 3349,\n",
       " 3356,\n",
       " 3429,\n",
       " 3504,\n",
       " 3545,\n",
       " 3553,\n",
       " 3559,\n",
       " 3573,\n",
       " 3621,\n",
       " 3622,\n",
       " 3623,\n",
       " 3625,\n",
       " 3635,\n",
       " 3639,\n",
       " 3659,\n",
       " 3665,\n",
       " 3666,\n",
       " 3685,\n",
       " 3726,\n",
       " 3733,\n",
       " 3739,\n",
       " 3766,\n",
       " 3768,\n",
       " 3774,\n",
       " 3784,\n",
       " 3792,\n",
       " 3812,\n",
       " 3816,\n",
       " 3839,\n",
       " 3847,\n",
       " 3852,\n",
       " 3859,\n",
       " 3930,\n",
       " 3932,\n",
       " 3933,\n",
       " 3943,\n",
       " 3948,\n",
       " 3952,\n",
       " 3968,\n",
       " 3984,\n",
       " 3985,\n",
       " 3986,\n",
       " 3987,\n",
       " 3988,\n",
       " 3989,\n",
       " 3990,\n",
       " 3995,\n",
       " 4007,\n",
       " 4028,\n",
       " 4029,\n",
       " 4030,\n",
       " 4044,\n",
       " 4071,\n",
       " 4078,\n",
       " 4083,\n",
       " 4084,\n",
       " 4101,\n",
       " 4103,\n",
       " 4105,\n",
       " 4123,\n",
       " 4124,\n",
       " 4125,\n",
       " 4126,\n",
       " 4127,\n",
       " 4128,\n",
       " 4129,\n",
       " 4130,\n",
       " 4131,\n",
       " 4132,\n",
       " 4133,\n",
       " 4138,\n",
       " 4151,\n",
       " 4152,\n",
       " 4158,\n",
       " 4181,\n",
       " 4188,\n",
       " 4213,\n",
       " 4221,\n",
       " 4232,\n",
       " 4244,\n",
       " 4264,\n",
       " 4265,\n",
       " 4266,\n",
       " 4276,\n",
       " 4278,\n",
       " 4279,\n",
       " 4292,\n",
       " 4295,\n",
       " 4299,\n",
       " 4309,\n",
       " 4310,\n",
       " 4318,\n",
       " 4320,\n",
       " 4322,\n",
       " 4334,\n",
       " 4342,\n",
       " 4362,\n",
       " 4368,\n",
       " 4369,\n",
       " 4371,\n",
       " 4373,\n",
       " 4374,\n",
       " 4375,\n",
       " 4380,\n",
       " 4381,\n",
       " 4382,\n",
       " 4384,\n",
       " 4385,\n",
       " 4386,\n",
       " 4387,\n",
       " 4388,\n",
       " 4389,\n",
       " 4390,\n",
       " 4391,\n",
       " 4392,\n",
       " 4393,\n",
       " 4397,\n",
       " 4407,\n",
       " 4408,\n",
       " 4409,\n",
       " 4410,\n",
       " 4418,\n",
       " 4420,\n",
       " 4427,\n",
       " 4438,\n",
       " 4443,\n",
       " 4445,\n",
       " 4446,\n",
       " 4447,\n",
       " 4448,\n",
       " 4458,\n",
       " 4462,\n",
       " 4464,\n",
       " 4466,\n",
       " 4482,\n",
       " 4483,\n",
       " 4489,\n",
       " 4504,\n",
       " 4527,\n",
       " 4542,\n",
       " 4545,\n",
       " 4546,\n",
       " 4548,\n",
       " 4550,\n",
       " 4552,\n",
       " 4553,\n",
       " 4554,\n",
       " 4568,\n",
       " 4569,\n",
       " 4596,\n",
       " 4604,\n",
       " 4605,\n",
       " 4608,\n",
       " 4614,\n",
       " 4615,\n",
       " 4618,\n",
       " 4620,\n",
       " 4621,\n",
       " 4624,\n",
       " 4625,\n",
       " 4632,\n",
       " 4633,\n",
       " 4637,\n",
       " 4642,\n",
       " 4644,\n",
       " 4645,\n",
       " 4646,\n",
       " 4647,\n",
       " 4653,\n",
       " 4654,\n",
       " 4655,\n",
       " 4656,\n",
       " 4657,\n",
       " 4660,\n",
       " 4668,\n",
       " 4673,\n",
       " 4678,\n",
       " 4681,\n",
       " 4684,\n",
       " 4685,\n",
       " 4689,\n",
       " 4690,\n",
       " 4693,\n",
       " 4709,\n",
       " 4712,\n",
       " 4713,\n",
       " 4720,\n",
       " 4721,\n",
       " 4725,\n",
       " 4726,\n",
       " 4727,\n",
       " 4730,\n",
       " 4731,\n",
       " 4732,\n",
       " 4733,\n",
       " 4734,\n",
       " 4737,\n",
       " 4745,\n",
       " 4746,\n",
       " 4747,\n",
       " 4748,\n",
       " 4758,\n",
       " 4772,\n",
       " 4773,\n",
       " 4778,\n",
       " 4779,\n",
       " 4780,\n",
       " 4781,\n",
       " 4796,\n",
       " 4797,\n",
       " 4798,\n",
       " 4799,\n",
       " 4800,\n",
       " 4809,\n",
       " 4811,\n",
       " 4813,\n",
       " 4814}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(simp_many).difference(set(trad_many))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"这里有两段故事——其中的一段是在柬埔寨。 我们选择的村庄没有电，没有自来水，没有电视，也没有电话， 但是现在却有了宽带互联网。\"\n",
    "translated = 'And these are two anecdotes -- one was in Cambodia, in a village that has no electricity, no water, no television, no telephone, but has broadband Internet now.'\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "trad_result = metric.compute(predictions=[lstm_trad.translate([converter.convert(sentence)])], references=[translated])\n",
    "simp_result = metric.compute(predictions=[lstm_simp.translate([sentence])], references=[translated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'這裏有兩段故事——其中的一段是在柬埔寨。 我們選擇的村莊沒有電，沒有自來水，沒有電視，也沒有電話， 但是現在卻有了寬帶互聯網。'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.convert(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tho paaieeetstt eao taieetso oodeaaietsee etaiaaeee  oodhtahn aotetc oteatothoelagg oteatothoelagg taieetso  ea1 tnst gtehre teetaieee  '"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = \"\"\n",
    "for word in converter.convert(\"夕阳下的湖面，波光粼粼的，十分好看。\"):\n",
    "    t += dic[word] + \" \"\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of indices: int64\n",
      "Data type of num_tokens_vec: int64\n",
      "Data type of max_tokens: <class 'int'>\n",
      "Data type of max_sentences: <class 'int'>\n",
      "Data type of bsz_mult: <class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Cambodia, Cambodia.'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = lstm_trad.translate([t])[0]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of indices: int64\n",
      "Data type of num_tokens_vec: int64\n",
      "Data type of max_tokens: <class 'int'>\n",
      "Data type of max_sentences: <class 'int'>\n",
      "Data type of bsz_mult: <class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This is a very good view of the lake of the laza of the lake.'"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = lstm_simp.translate([t])[0]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simp_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['taetno', 'teadeaeero', 'etaeae', 'eaaef', 'lldiec', 'ohatost', 'lthno', 'aieetsee', 'hr', 'aieec', 'oodsac', 'tteaie', 'thnaie', 'aieeaeoe', 'tatc', 'ootohhre', 'oohetktttn', 'lldtnst', 'lldtnst', 'teatoar', 'tseea', 'aieec', 'oodsac', 'tnst', 'thtseeastteea', 'rhtn', 'etsuto', 'eadst', 'eaaaietn', 'aieeaeoe', 'tatc', 'ootohhre', 'oohetktttn', 'oodhn', 'iebta', 'aieeaee1', 'eaeaieaeeaaetouto', 'lldtaoeeeae', 'aahneae', 'erdeaeero', 'etao', 'ooa@@', 'thtseeastteea', 'eadsac', 'totn1', 'eaieieer', 'eer1', 'teatoar', 'tseea', 'aieec', 'oodsac', 'tnst', 'thtseeastteea', 'thnaie', 'teatoaiea', 'teadtstt', 'tteaaito', 'taieetso', 'oohooeto', 'ibdtneoote', 'etaieeeta', 'oohtotv']\n",
      "伏\n",
      "特\n",
      "在\n",
      "世\n",
      "纪\n",
      "初\n",
      "发\n",
      "明\n",
      "了\n",
      "电\n",
      "池\n",
      "后\n",
      "各\n",
      "国\n",
      "化\n",
      "学\n",
      "家\n",
      "纷\n",
      "纷\n",
      "利\n",
      "用\n",
      "电\n",
      "池\n",
      "分\n",
      "解\n",
      "水\n",
      "成\n",
      "功\n",
      "英\n",
      "国\n",
      "化\n",
      "学\n",
      "家\n",
      "汉\n",
      "弗\n",
      "里\n",
      "戴\n",
      "维\n",
      "坚\n",
      "持\n",
      "不\n",
      "忄\n",
      "解\n",
      "地\n",
      "从\n",
      "事\n",
      "于\n",
      "利\n",
      "用\n",
      "电\n",
      "池\n",
      "分\n",
      "解\n",
      "各\n",
      "种\n",
      "物\n",
      "质\n",
      "的\n",
      "实\n",
      "验\n",
      "研\n",
      "究\n"
     ]
    }
   ],
   "source": [
    "original = \"\".join(sample.split()).replace(\"@\", \"\")\n",
    "print(st2zh[original])\n",
    "splits = sample.split()\n",
    "print(splits)\n",
    "for split in splits:\n",
    "    print(st2zh[split.strip(\"@\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of indices: int64\n",
      "Data type of num_tokens_vec: int64\n",
      "Data type of max_tokens: <class 'int'>\n",
      "Data type of max_sentences: <class 'int'>\n",
      "Data type of bsz_mult: <class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['And in the beginning of the first time in the beginning of the 20th century, the chemists were using batteries to be able to cut the water from the batteries to the British National Academy of UK in the United Kingdom, and in the United Kingdom of the United Kingdom.']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_simp.translate([strokes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toeeaotd@@ as@@ to aieeeaetn e teatoteaieeeae tneeaote ietaodtoaiaaetsaieaeo eetc0 eeaellorto '"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_trad.apply_bpe(strokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of indices: int64\n",
      "Data type of num_tokens_vec: int64\n",
      "Data type of max_tokens: <class 'int'>\n",
      "Data type of max_sentences: <class 'int'>\n",
      "Data type of bsz_mult: <class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"It's a metal metal.\"]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_trad.translate([strokes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trad_strokes = []\n",
    "simp_strokes = []\n",
    "\n",
    "for word in list(st2zh.keys()):\n",
    "    strokes = lstm_simp.apply_bpe(word)\n",
    "    splits = strokes.split()\n",
    "    if len(splits) > 1: simp_strokes.append(strokes)\n",
    "    strokes = lstm_trad.apply_bpe(word)\n",
    "    splits = strokes.split()\n",
    "    if len(splits) > 1: trad_strokes.append(strokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "踴\n",
      "['aieaead@@', 'hoaieeast']\n",
      "\n",
      "勇\n",
      "諃\n",
      "['oee@@', 'eaiee@@', 'ato@@', 'eatn']\n",
      "\n",
      "\n",
      "\n",
      "木\n",
      "惪\n",
      "['eaaieeee@@', 'ouoo']\n",
      "直\n",
      "心\n",
      "穸\n",
      "['oohto@@', 'tho']\n",
      "\n",
      "夕\n",
      "餪\n",
      "['tooi@@', 'eem@@', 'oe@@', 'tas@@', 'aa@@', 'etn']\n",
      "\n",
      "\n",
      "亠\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "samples = random.sample(simp_strokes, 5)\n",
    "for sample in samples:\n",
    "    original = \"\".join(sample.split()).replace(\"@\", \"\")\n",
    "    print(st2zh[original])\n",
    "    splits = sample.split()\n",
    "    print(splits)\n",
    "    for split in splits:\n",
    "        print(st2zh[split.strip(\"@\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7148,  2185,  3987, 13492, 20226, 13154,     2])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_simp.encode(\"eaioteatn eadeaseeao ooheeaaetnerto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19242,  4234, 13588, 20531, 14413,     2])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_trad.encode(\"eaioteatn eadeaseeao ooheeaaetnerto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st2zh[\"eaaeedtetn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of indices: int64\n",
      "Data type of num_tokens_vec: int64\n",
      "Data type of max_tokens: <class 'int'>\n",
      "Data type of max_sentences: <class 'int'>\n",
      "Data type of bsz_mult: <class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['She was her her. She was her.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_simp.translate([\"gtesac\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of indices: int64\n",
      "Data type of num_tokens_vec: int64\n",
      "Data type of max_tokens: <class 'int'>\n",
      "Data type of max_sentences: <class 'int'>\n",
      "Data type of bsz_mult: <class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['She was her. She was her.']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_trad.translate([\"gtesac\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xk_20\\AppData\\Local\\Temp\\ipykernel_18980\\923119910.py:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  tokens = torch.tensor([tokens.numpy()])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "sentence = \"aeeeaeee aotohaieasa oytaseeetero oytaseeetero ， eaaietc aieeaee1 eaaeeetottea 。 taieetso etaiethtseea aeeeaeee aotohaieasa eaaoheatn eaeoteea loaseetctc etasee teoteoiebat ee odthtn oeotdaeaie etaeae oetoown tna loaie aee taieetso eatotv1 tneelo ， terduto eaaieeeeto aieeeaetn aeeeaeee aotohaieasa eteaieutoouoo oodtaieeoesttetn 。\"\n",
    "\n",
    "# Get token ids\n",
    "tokens = lstm.encode(sentence)\n",
    "token_len = len(tokens)\n",
    "\n",
    "# Convert to input requirement\n",
    "tokens = torch.tensor([tokens.numpy()])\n",
    "\n",
    "# get encoder\n",
    "encoder = lstm.get_submodule(\"models.0.encoder\")\n",
    "\n",
    "# get output\n",
    "encoded = encoder(tokens, torch.Tensor([token_len]))\n",
    "\n",
    "attention = lstm.get_submodule(\"models.0.decoder.attention\")\n",
    "decoder = lstm.get_submodule(\"models.0.decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMEncoder(\n",
       "  (dropout_in_module): FairseqDropout()\n",
       "  (dropout_out_module): FairseqDropout()\n",
       "  (embed_tokens): Embedding(23329, 512, padding_idx=1)\n",
       "  (lstm): LSTM(512, 512)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.3972,  0.1230,  0.0320,  ..., -0.2124, -0.0334,  0.3662],\n",
       "         [ 0.7861,  0.0558, -0.1696,  ...,  0.5332,  0.2297,  0.1222],\n",
       "         [ 0.1328,  0.0801, -0.2139,  ..., -0.0578,  0.0297,  0.1720],\n",
       "         ...,\n",
       "         [-0.0952,  0.0209,  0.0868,  ...,  0.0665, -0.1107, -0.0257],\n",
       "         [ 0.0182,  0.2108, -0.1046,  ...,  0.1218, -0.1348,  0.0806],\n",
       "         [ 0.2224,  0.0419, -0.0886,  ..., -0.0465, -0.1129, -0.3384]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0368,  0.1473,  0.1625,  ..., -0.2269,  0.0818, -0.3306],\n",
       "         [-0.0143, -0.1036,  0.0038,  ..., -0.0629,  0.1381, -0.1331],\n",
       "         [-0.0342,  0.0387,  0.0382,  ..., -0.0023, -0.1389,  0.0848],\n",
       "         ...,\n",
       "         [ 0.2449,  0.0371, -0.1398,  ...,  0.0485,  0.0908,  0.0790],\n",
       "         [ 0.1252,  0.2656, -0.0487,  ..., -0.1017, -0.5576,  0.2063],\n",
       "         [ 0.2178, -0.1361, -0.0264,  ...,  0.0380,  0.1033, -0.0215]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0287, -0.0757, -0.0850,  ...,  0.1091,  0.0482, -0.0473],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1116, -0.1003, -0.0579,  ...,  0.1533,  0.0134, -0.0259],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM weights\n",
    "list(encoder.get_submodule(\"lstm\").parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0626, -0.0757,  0.0169,  ..., -0.0498,  0.1516, -0.0409],\n",
       "         [ 0.0626, -0.0757,  0.0169,  ..., -0.0498,  0.1516, -0.0409],\n",
       "         [-0.1069, -0.0035, -0.0381,  ...,  0.0343,  0.2073, -0.0552],\n",
       "         ...,\n",
       "         [ 0.0626, -0.0757,  0.0169,  ..., -0.0498,  0.1516, -0.0409],\n",
       "         [ 0.0626, -0.0757,  0.0169,  ..., -0.0498,  0.1516, -0.0409],\n",
       "         [-0.0088,  0.0045, -0.0016,  ...,  0.0061, -0.0041, -0.0017]],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token Embeddings weights embedding[token_id] = token_embedding\n",
    "embeddings = list(encoder.get_submodule(\"embed_tokens\").parameters())\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "非常谢谢克里斯的确非常荣幸能有第二次站在这个台上的机会我真是非常感激"
     ]
    }
   ],
   "source": [
    "for word in sentence.split():\n",
    "    print(st2zh[word], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = []\n",
    "words = []\n",
    "for token in tokens[0]:\n",
    "    embeds.append(embeddings[0][token.numpy()])\n",
    "    words.append(st2zh[lstm.decode(torch.tensor([token]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gte'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aeeeaeee'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.decode(torch.tensor([tokens[0][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0436, -0.0187, -0.0006,  ..., -0.0314, -0.0314, -0.0040],\n",
       "        [-0.0151, -0.0518, -0.0054,  ...,  0.0010, -0.0205,  0.0126],\n",
       "        [ 0.0066,  0.0182, -0.0159,  ..., -0.0102,  0.0087, -0.0757],\n",
       "        ...,\n",
       "        [ 0.0047, -0.0118, -0.0303,  ...,  0.0219, -0.0080, -0.0251],\n",
       "        [ 0.0124,  0.0779, -0.0190,  ...,  0.0056, -0.0512,  0.0226],\n",
       "        [-0.0121,  0.0174, -0.0286,  ...,  0.0104,  0.0126, -0.0388]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.get_submodule(\"input_proj\").weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 3.5828e-02,  1.1627e-02,  3.9612e-02,  ...,  3.2013e-02,\n",
       "         -1.0663e-01, -1.1843e-04],\n",
       "        [ 3.5980e-02, -3.6841e-01,  2.8671e-02,  ...,  5.2547e-04,\n",
       "         -1.3989e-01, -3.4131e-01],\n",
       "        [-1.3818e-01, -2.4268e-01, -2.3308e-03,  ..., -1.3867e-01,\n",
       "          9.1064e-02, -2.7847e-02],\n",
       "        ...,\n",
       "        [-1.9788e-01, -2.7246e-01,  8.2092e-02,  ...,  5.2856e-02,\n",
       "         -1.5222e-01, -8.3740e-02],\n",
       "        [ 1.1093e-02,  1.1420e-01, -1.5381e-02,  ..., -8.8959e-03,\n",
       "          1.3049e-01,  1.2769e-01],\n",
       "        [ 7.1068e-03, -1.7334e-01, -8.5815e-02,  ...,  1.6479e-02,\n",
       "         -4.0924e-02, -6.8665e-02]], requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.get_submodule(\"output_proj\").weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMEncoder(\n",
       "  (dropout_in_module): FairseqDropout()\n",
       "  (dropout_out_module): FairseqDropout()\n",
       "  (embed_tokens): Embedding(23329, 512, padding_idx=1)\n",
       "  (lstm): LSTM(512, 512)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMDecoder(\n",
       "  (dropout_in_module): FairseqDropout()\n",
       "  (dropout_out_module): FairseqDropout()\n",
       "  (embed_tokens): Embedding(23329, 512, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0): LSTMCell(1024, 512)\n",
       "  )\n",
       "  (attention): AttentionLayer(\n",
       "    (input_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (output_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"aieeaee1 aieeaee1 asdasdasf\"\n",
    "s = lstm.tokenize(sentence)\n",
    "a = lstm.apply_bpe(s)\n",
    "lstm.binarize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s,a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".TFT_OCR_BOT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
