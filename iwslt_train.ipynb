{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"iwslt2017\"\n",
    "dataset = load_dataset(dataset_name, \"iwslt2017-zh-en\", cache_dir=\"./cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opencc\n",
    "converter = opencc.OpenCC('s2t.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "word_freq = defaultdict(int)\n",
    "for pair in dataset[\"train\"][\"translation\"]:\n",
    "    for word in pair[\"zh\"]:\n",
    "        word_freq[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# Create dict for text into strokes translation and vice versa\n",
    "with open(\"./vocab/zh2letter.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    conversions = f.read()\n",
    "\n",
    "conversions = conversions.splitlines()\n",
    "dic = defaultdict(str)\n",
    "for line in conversions:\n",
    "    chinese_char, strokes = line.split()\n",
    "    dic[chinese_char] = strokes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strokify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def is_chinese(uchar):\n",
    "    \"\"\"判断一个unicode是否是汉字\"\"\"\n",
    "    if (uchar >= u'\\u4e00') and (uchar <= u'\\u9fa5'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def zh2letter(dictionary, line):\n",
    "    char_set = set(list(line))\n",
    "    newline = line\n",
    "    for char in char_set:\n",
    "        if is_chinese(char):\n",
    "            newline = newline.replace(char, ' '+dictionary.get(char, '')+' ')\n",
    "    return ' '.join(newline.split())+'\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_freq = [(w,f) for w,f in sorted(word_freq.items(), key=lambda x: x[1], reverse=True) if is_chinese(w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_len = 0\n",
    "for w,f in sorted_freq:\n",
    "    avg_len += len(dic[w])\n",
    "avg_len / len(sorted_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_len = 0\n",
    "for w,f in sorted_freq:\n",
    "    avg_len += len(dic[converter.convert(w)])\n",
    "avg_len / len(sorted_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./frequency.txt\", 'w', encoding='utf-8') as f:\n",
    "    for word,freq in sorted_freq:\n",
    "        f.write(f'{word} {freq} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPES = [\"zh\", \"tz\"]\n",
    "NAMES = [\"simp\", \"trad\"]\n",
    "TYPE = 0 # 0 for simplified, 1 for traditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split=\"train\"\n",
    "if TYPE == 0:\n",
    "    src_text = [pair[\"zh\"] for pair in dataset[split][\"translation\"]]\n",
    "else:\n",
    "    src_text = [converter.convert(pair[\"zh\"]) for pair in dataset[split][\"translation\"]]\n",
    "trg_text = [pair[\"en\"] for pair in dataset[split][\"translation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "src = TYPES[TYPE]\n",
    "trg = \"en\"\n",
    "\n",
    "func = partial(zh2letter, dic)\n",
    "iter = map(func, src_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"./data/NIST/{NAMES[TYPE]}\"\n",
    "with open(f\"{path}/{split}.{src}-{trg}.{src}\", 'w', encoding=\"utf-8\") as f:\n",
    "    for k in tqdm(iter): f.write(k)\n",
    "\n",
    "with open(f\"{path}/{split}.{src}-{trg}.{trg}\", 'w', encoding=\"utf-8\") as f:\n",
    "    for k in tqdm(trg_text): f.write(f\"{k}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cypher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_vocab(vocab, key):\n",
    "    dic = {}\n",
    "    for i in range(len(vocab)):\n",
    "        dic[vocab[i]] = vocab[(i+key) % len(vocab)]\n",
    "    return dic\n",
    "\n",
    "def monophonic(vocab, shifted_vocab, plain_text):\n",
    "    cipher_text = []\n",
    "    for c in plain_text:\n",
    "        if c in vocab:\n",
    "            cipher_text.append(shifted_vocab[c])\n",
    "        else:\n",
    "            cipher_text.append(c)\n",
    "    return ''.join(cipher_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(path):\n",
    "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "        text = f.readlines()\n",
    "    return text\n",
    "\n",
    "def write_file(src, trg):\n",
    "    with open(src, 'r',encoding=\"utf-8\") as f1, open(trg, 'w',encoding=\"utf-8\") as f2:\n",
    "        for k in f1.readlines(): f2.write(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "splits = ['train', 'validation']\n",
    "vocab = 'etaoinshrdlcumwfgypbvkjxqz'\n",
    "src = TYPES[TYPE]\n",
    "trg = \"en\"\n",
    "\n",
    "# for split in splits:\n",
    "for split in [\"test\"]:\n",
    "    src_name = f\"{split}.{src}-{trg}.{src}\"\n",
    "    trg_name = f\"{split}.{src}-{trg}.{trg}\"\n",
    "    src_file = os.path.join(path, src_name)\n",
    "    trg_file = os.path.join(path, trg_name)\n",
    "    text = read_text(src_file)\n",
    "    for key in [1,2]:\n",
    "        shifted_vocab = shift_vocab(vocab, key)\n",
    "        func = partial(monophonic, vocab, shifted_vocab)\n",
    "        print(f'Generating ciphered-text of {split} data with key {key}.')\n",
    "        write_file(trg_file, os.path.join(path, f\"{split}.{src}{key}-{trg}.{trg}\"))\n",
    "        save_src_cipher = os.path.join(path, f\"{split}.{src}{key}-{trg}.{src}{key}\")\n",
    "        \n",
    "        iter = map(func, text)\n",
    "        with open(save_src_cipher, 'w', encoding=\"utf-8\") as f:\n",
    "            for k in tqdm(iter): f.write(k)\n",
    "        \n",
    "        write_file(src_file, os.path.join(path, f\"{split}.{src}{key}-{src}.{src}\"))\n",
    "        write_file(save_src_cipher, os.path.join(path, f\"{split}.{src}{key}-{src}.{src}{key}\"))\n",
    "        print('done \\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".TFT_OCR_BOT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
