{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataset for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./chinese.zh\", 'r', encoding=\"utf-8\") as f:\n",
    "    src_text = f.read().splitlines()\n",
    "\n",
    "with open(\"./english.en\", 'r', encoding=\"utf-8\") as f:\n",
    "    trg_text = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "word_freq = defaultdict(int)\n",
    "for sent in src_text:\n",
    "    for word in sent:\n",
    "        word_freq[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(src_text), len(trg_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# Create dict for text into strokes translation and vice versa\n",
    "with open(\"./vocab/zh2letter.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    conversions = f.read()\n",
    "\n",
    "conversions = conversions.splitlines()\n",
    "dic = defaultdict(str)\n",
    "st2zh = defaultdict(str)\n",
    "for line in conversions:\n",
    "    chinese_char, strokes = line.split()\n",
    "    dic[chinese_char] = strokes\n",
    "    st2zh[strokes] = chinese_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strokify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def is_chinese(uchar):\n",
    "    \"\"\"判断一个unicode是否是汉字\"\"\"\n",
    "    if (uchar >= u'\\u4e00') and (uchar <= u'\\u9fa5'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def zh2letter(dictionary, line):\n",
    "    char_set = set(list(line))\n",
    "    newline = line\n",
    "    for char in char_set:\n",
    "        if is_chinese(char):\n",
    "            newline = newline.replace(char, ' '+dictionary.get(char, '')+' ')\n",
    "    return ' '.join(newline.split())+'\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_freq = [(w,f) for w,f in sorted(word_freq.items(), key=lambda x: x[1], reverse=True) if is_chinese(w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./frequency_news.txt\", 'w', encoding='utf-8') as f:\n",
    "    for word,freq in sorted_freq:\n",
    "        f.write(f'{word} {freq} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./frequency.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    iwslt_words = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iwslt_word = []\n",
    "for word in iwslt_words:\n",
    "    iwslt_word.append(word.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_words = []\n",
    "for word,f in sorted_freq:\n",
    "    news_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(news_words).difference(set(iwslt_word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(iwslt_word).difference(set(news_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPES = [\"zh\", \"tz\"]\n",
    "NAMES = [\"simp\", \"trad\"]\n",
    "TYPE = 0 # 0 for simplified, 1 for traditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opencc\n",
    "converter = opencc.OpenCC('s2t.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "src = TYPES[TYPE]\n",
    "trg = \"en\"\n",
    "\n",
    "func = partial(zh2letter, dic)\n",
    "iter = map(func, src_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"./data/NIST/{NAMES[TYPE]}/test/news\"\n",
    "split=\"test\"\n",
    "with open(f\"{path}/{split}.{src}-{trg}.{src}\", 'w', encoding=\"utf-8\") as f:\n",
    "    for k in tqdm(iter): f.write(k)\n",
    "\n",
    "with open(f\"{path}/{split}.{src}-{trg}.{trg}\", 'w', encoding=\"utf-8\") as f:\n",
    "    for k in tqdm(trg_text): f.write(f\"{k}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split by length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(zh) for zh in src_text]\n",
    "import numpy as np\n",
    "for p in [33, 66]:\n",
    "    print(np.percentile(lens, p))\n",
    "# Split by length\n",
    "from collections import defaultdict\n",
    "sentence_by_length = defaultdict(list)\n",
    "for idx in range(len(src_text)):\n",
    "    pair = dict()\n",
    "    pair[\"zh\"] = src_text[idx]\n",
    "    pair[\"en\"] = trg_text[idx]\n",
    "    if len(pair[\"zh\"]) <= 18:\n",
    "        sentence_by_length[\"short\"].append(pair)\n",
    "    elif len(pair[\"zh\"]) <= 33:\n",
    "        sentence_by_length[\"medium\"].append(pair)\n",
    "    else:\n",
    "        sentence_by_length[\"long\"].append(pair)\n",
    "for type, sent in sentence_by_length.items():\n",
    "    print(type, len(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split=\"test\"\n",
    "\n",
    "src_text = defaultdict(list)\n",
    "trg_text = defaultdict(list)\n",
    "for type, sent in sentence_by_length.items():\n",
    "    src_text[type] = [pair[\"zh\"] for pair in sent]\n",
    "    trg_text[type] = [pair[\"en\"] for pair in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = f\"./data/NIST/simp/test/news_sent_sampled\"\n",
    "os.makedirs(path) if not os.path.exists(path) else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since theres too many sentences and it takes too long to evaluate all, we only sample out to get an exepected size of 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sampled_src_text = defaultdict(list)\n",
    "sampled_trg_text = defaultdict(list)\n",
    "for word, pairs in sentence_by_length.items():\n",
    "    k = 1000\n",
    "    samples = random.sample(range(0, len(pairs)), k)\n",
    "    for sample in samples:\n",
    "        sampled_src_text[word].append(pairs[sample][\"zh\"])\n",
    "        sampled_trg_text[word].append(pairs[sample][\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sentence_by_length.keys():\n",
    "    iter = map(func, sampled_src_text[word])\n",
    "    with open(f\"{path}/{split}-{word}.{src}-{trg}.{src}\", 'w', encoding=\"utf-8\") as f:\n",
    "        for k in tqdm(iter): f.write(k)\n",
    "\n",
    "    with open(f\"{path}/{split}-{word}.{src}-{trg}.{trg}\", 'w', encoding=\"utf-8\") as f:\n",
    "        for k in tqdm(sampled_trg_text[word]): f.write(f\"{k}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "TYPES = [\"zh\", \"tz\"]\n",
    "NAMES = [\"simp\", \"trad\"]\n",
    "TYPE = 0 # 0 for simplified, 1 for traditional\n",
    "\n",
    "src = TYPES[TYPE]\n",
    "trg = \"en\"\n",
    "\n",
    "func = partial(zh2letter, dic)\n",
    "iter = map(func, src_text)\n",
    "\n",
    "strokes = []\n",
    "for k in tqdm(iter):\n",
    "    strokes.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "avg_token_len = []\n",
    "for sent in strokes:\n",
    "    words = sent.split(\" \")\n",
    "    stroke_len = [len(word) for word in words]\n",
    "    avg_token_len.append(np.average(stroke_len))\n",
    "for p in [33, 66]:\n",
    "    print(np.percentile(avg_token_len, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_avg_strokes = defaultdict(list)\n",
    "trg_avg_strokes = defaultdict(list)\n",
    "for i, l in enumerate(avg_token_len):\n",
    "    if l <= 6.785714285714286:\n",
    "        src_avg_strokes[\"short\"].append(src_text[i])\n",
    "        trg_avg_strokes[\"short\"].append(trg_text[i])\n",
    "    elif l <= 7.2407407407407405:\n",
    "        src_avg_strokes[\"medium\"].append(src_text[i])\n",
    "        trg_avg_strokes[\"medium\"].append(trg_text[i])\n",
    "    else:\n",
    "        src_avg_strokes[\"long\"].append(src_text[i])\n",
    "        trg_avg_strokes[\"long\"].append(trg_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sampled_src_text = defaultdict(list)\n",
    "sampled_trg_text = defaultdict(list)\n",
    "for word, pairs in src_avg_strokes.items():\n",
    "    k = 1000\n",
    "    samples = random.sample(range(0, len(pairs)), k)\n",
    "    for sample in samples:\n",
    "        sampled_src_text[word].append(src_avg_strokes[word][sample])\n",
    "        sampled_trg_text[word].append(trg_avg_strokes[word][sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sampled_src_text.values():\n",
    "    print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\".data/NIST/simp/test/news_stroke\"\n",
    "split=\"test\"\n",
    "for word in sampled_src_text.keys():\n",
    "    iter = map(func, sampled_src_text[word])\n",
    "    with open(f\"{path}/{split}-{word}.{src}-{trg}.{src}\", 'w', encoding=\"utf-8\") as f:\n",
    "        for k in tqdm(iter): f.write(k)\n",
    "\n",
    "    with open(f\"{path}/{split}-{word}.{src}-{trg}.{trg}\", 'w', encoding=\"utf-8\") as f:\n",
    "        for k in tqdm(sampled_trg_text[word]): f.write(f\"{k}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".TFT_OCR_BOT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
