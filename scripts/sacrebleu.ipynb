{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"all\", \"news\", \"news_sent_sampled/short\", \"news_sent_sampled/medium\", \"news_sent_sampled/long\", \"news_stroke/short\", \"news_stroke/medium\", \"news_stroke/long\", \"sent/short\", \"sent/medium\", \"sent/long\", \"stroke/short\", \"stroke/medium\", \"stroke/long\"]\n",
    "datasets = [\"all\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"zh\"\n",
    "dataset = \"simp\"\n",
    "for d in datasets:\n",
    "    print(d)\n",
    "    with open(f\"../results/{dataset}/{d}/generate-test.hyp\", 'r', encoding='utf-8') as f:\n",
    "        preds = f.read().splitlines()\n",
    "    \n",
    "    splits = d.split(\"/\")\n",
    "    if len(splits) > 1:\n",
    "        with open(f\"../data/NIST/{dataset}/test/{splits[0]}/test-{splits[1]}.{src}-en.en\", 'r', encoding='utf-8') as f:\n",
    "            refs = f.read().splitlines()\n",
    "    else:\n",
    "        with open(f\"../data/NIST/{dataset}/test/{d}/test.{src}-en.en\", 'r', encoding='utf-8') as f:\n",
    "            refs = f.read().splitlines()\n",
    "    \n",
    "    result = metric.compute(predictions=preds, references=refs)\n",
    "    print(result)\n",
    "    result = {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in datasets:\n",
    "    print(d)\n",
    "    with open(f\"../results/trad/{d}/generate-test.hyp\", 'r', encoding='utf-8') as f:\n",
    "        trad_preds = f.read().splitlines()\n",
    "    with open(f\"../results/simp/{d}/generate-test.hyp\", 'r', encoding='utf-8') as f:\n",
    "        simp_preds = f.read().splitlines()\n",
    "    \n",
    "    splits = d.split(\"/\")\n",
    "    with open(f\"../data/NIST/trad/test/{d}/test.tz-en.en\", 'r', encoding='utf-8') as f:\n",
    "        trad_refs = f.read().splitlines()\n",
    "    with open(f\"../data/NIST/simp_original/test/{d}/test.zh-en.en\", 'r', encoding='utf-8') as f:\n",
    "        simp_refs = f.read().splitlines()\n",
    "    \n",
    "    cnt = 0\n",
    "    for i in range(len(trad_preds)):\n",
    "        trad_result = metric.compute(predictions=[trad_preds[i]], references=[trad_refs[i]])\n",
    "        simp_result = metric.compute(predictions=[simp_preds[i]], references=[simp_refs[i]])\n",
    "        THRESHOLD = 15\n",
    "        if simp_result[\"score\"] - trad_result[\"score\"] > THRESHOLD:\n",
    "            # print(i)\n",
    "            # print(\"simp better\")\n",
    "            print(simp_preds[i], '&', trad_preds[i], '&', trad_refs[i], '\\\\\\\\')\n",
    "            cnt += 1\n",
    "        elif simp_result[\"score\"] - trad_result[\"score\"] < -THRESHOLD:\n",
    "            # print(i)\n",
    "            print(\"trad better\")\n",
    "            print(simp_preds[i], '&', trad_preds[i], '&', trad_refs[i], '\\\\\\\\')\n",
    "            cnt += 1\n",
    "        if (cnt % 10 == 0):\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".TFT_OCR_BOT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
